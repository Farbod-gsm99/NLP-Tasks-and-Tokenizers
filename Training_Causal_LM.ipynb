{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPakigI7nEUH67nzLXvMuo8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Farbod-gsm99/NLP-Tasks-and-Tokenizers/blob/main/Training_Causal_LM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_1lF0RDwqxrs"
      },
      "outputs": [],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def any_keyword_in_string(string, keywords):\n",
        "    for keyword in keywords:\n",
        "        if keyword in string:\n",
        "            return True\n",
        "    return False"
      ],
      "metadata": {
        "id": "CPBvS4Rmsrd-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "from datasets import Dataset\n",
        "\n",
        "\n",
        "def filter_streaming_dataset(dataset, filters):\n",
        "    filtered_dict = defaultdict(list)\n",
        "    total = 0\n",
        "    for sample in tqdm(iter(dataset)):\n",
        "        total += 1\n",
        "        if any_keyword_in_string(sample[\"content\"], filters):\n",
        "            for k, v in sample.items():\n",
        "                filtered_dict[k].append(v)\n",
        "    print(f\"{len(filtered_dict['content'])/total:.2%} of data after filtering.\")\n",
        "    return Dataset.from_dict(filtered_dict)"
      ],
      "metadata": {
        "id": "LOHOaPEBssD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Raw Dataset"
      ],
      "metadata": {
        "id": "9VzdIyJjwRwr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "split = \"train\"  # \"valid\"\n",
        "filters = [\"pandas\", \"sklearn\", \"matplotlib\", \"seaborn\"]\n",
        "\n",
        "data = load_dataset(f\"transformersbook/codeparrot-{split}\", split=split, streaming=True)\n",
        "filtered_data = filter_streaming_dataset(data, filters)"
      ],
      "metadata": {
        "id": "i2fL43opraD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Filtered Dataset"
      ],
      "metadata": {
        "id": "7xFSBiI_waNL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, DatasetDict\n",
        "\n",
        "ds_train = load_dataset(\"huggingface-course/codeparrot-ds-train\", split=\"train\")\n",
        "ds_valid = load_dataset(\"huggingface-course/codeparrot-ds-valid\", split=\"validation\")\n",
        "\n",
        "raw_datasets = DatasetDict(\n",
        "    {\n",
        "        \"train\": ds_train,  # .shuffle().select(range(50000)),\n",
        "        \"valid\": ds_valid,  # .shuffle().select(range(500))\n",
        "    }\n",
        ")\n",
        "\n",
        "raw_datasets"
      ],
      "metadata": {
        "id": "MUEz7OjFwNkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "context_length = 128\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"huggingface-course/code-search-net-tokenizer\")\n",
        "\n",
        "outputs = tokenizer(\n",
        "    raw_datasets[\"train\"][:2][\"content\"],\n",
        "    truncation=True,\n",
        "    max_length=context_length,\n",
        "    return_overflowing_tokens=True,\n",
        "    return_length=True,\n",
        ")\n",
        "\n",
        "print(f\"Input IDs length: {len(outputs['input_ids'])}\")\n",
        "print(f\"Input chunk lengths: {(outputs['length'])}\")\n",
        "print(f\"Chunk mapping: {outputs['overflow_to_sample_mapping']}\")"
      ],
      "metadata": {
        "id": "c-CrN_nwwcp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(element):\n",
        "    outputs = tokenizer(\n",
        "        element[\"content\"],\n",
        "        truncation=True,\n",
        "        max_length=context_length,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_length=True,\n",
        "    )\n",
        "    input_batch = []\n",
        "    for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n",
        "        if length == context_length:\n",
        "            input_batch.append(input_ids)\n",
        "    return {\"input_ids\": input_batch}\n",
        "\n",
        "\n",
        "tokenized_datasets = raw_datasets.map(\n",
        "    tokenize, batched=True, remove_columns=raw_datasets[\"train\"].column_names\n",
        ")\n",
        "tokenized_datasets"
      ],
      "metadata": {
        "id": "57_40IbvxHwM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig\n",
        "\n",
        "config = AutoConfig.from_pretrained(\n",
        "    \"gpt2\",\n",
        "    vocab_size=len(tokenizer),\n",
        "    n_ctx=context_length,\n",
        "    bos_token_id=tokenizer.bos_token_id,\n",
        "    eos_token_id=tokenizer.eos_token"
      ],
      "metadata": {
        "id": "AzGgm3_0xWmj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPT2LMHeadModel(config)\n",
        "model_size = sum(t.numel() for t in model.parameters())\n",
        "print(f\"GPT-2 size: {model_size/1000**2:.1f}M parameters\")"
      ],
      "metadata": {
        "id": "68mWdXVI32XV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
      ],
      "metadata": {
        "id": "Ot1RK5eV34V8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "LJ_93prt5x_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"codeparrot-ds\",\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=5_000,\n",
        "    logging_steps=5_000,\n",
        "    gradient_accumulation_steps=8,\n",
        "    num_train_epochs = 1,\n",
        "    weight_decay=0.1,\n",
        "    warmup_steps=1_000,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    learning_rate=5e-4,\n",
        "    save_steps=1_000,\n",
        "    fp16=True,\n",
        "    push_to_hub=True,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    args=args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"valid\"],\n",
        ")"
      ],
      "metadata": {
        "id": "YtCHnePq5yfn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "Fo1Gqi4_DiRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.push_to_hub()"
      ],
      "metadata": {
        "id": "SWCGBFyBDjph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import pipeline\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "pipe = pipeline(\n",
        "    \"text-generation\", model=\"TheFuriousGunner/codeparrot-ds\", device=device\n",
        ")"
      ],
      "metadata": {
        "id": "Lq4yppLXTgFY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}