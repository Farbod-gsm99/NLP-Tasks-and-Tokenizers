{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPYPpAGUTsCssfB8aJVriO5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Farbod-gsm99/NLP-Tasks-and-Tokenizers/blob/main/Training_Unigram_Tokenizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uwLkDwLpZzs0"
      },
      "outputs": [],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import (\n",
        "    decoders,\n",
        "    models,\n",
        "    normalizers,\n",
        "    pre_tokenizers,\n",
        "    processors,\n",
        "    trainers,\n",
        "    Tokenizer,\n",
        ")\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"wikitext\", name=\"wikitext-2-raw-v1\", split=\"train\")\n",
        "\n",
        "\n",
        "def get_training_corpus():\n",
        "    for i in range(0, len(dataset), 1000):\n",
        "        yield dataset[i : i + 1000][\"text\"]"
      ],
      "metadata": {
        "id": "Ji4LnFMOZ44f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(models.Unigram())"
      ],
      "metadata": {
        "id": "0qt1QXJhZ78Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Regex\n",
        "\n",
        "tokenizer.normalizer = normalizers.Sequence(\n",
        "    [\n",
        "        normalizers.Replace(\"``\", '\"'),\n",
        "        normalizers.Replace(\"''\", '\"'),\n",
        "        normalizers.NFKD(),\n",
        "        normalizers.StripAccents(),\n",
        "        normalizers.Replace(Regex(\" {2,}\"), \" \"),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "RHj-rNPLaB3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.pre_tokenizer = pre_tokenizers.Metaspace()"
      ],
      "metadata": {
        "id": "cYgMlgcJaCuQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "special_tokens = [\"<cls>\", \"<sep>\", \"<unk>\", \"<pad>\", \"<mask>\", \"<s>\", \"</s>\"]\n",
        "trainer = trainers.UnigramTrainer(\n",
        "    vocab_size=25000, special_tokens=special_tokens, unk_token=\"<unk>\"\n",
        ")\n",
        "tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)\n",
        "encoding = tokenizer.encode(\"Let's test this tokenizer.\")\n",
        "print(encoding.tokens)"
      ],
      "metadata": {
        "id": "BpCbNNL2aFhv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cls_token_id = tokenizer.token_to_id(\"<cls>\")\n",
        "sep_token_id = tokenizer.token_to_id(\"<sep>\")\n",
        "print(cls_token_id, sep_token_id)"
      ],
      "metadata": {
        "id": "_4bOBrxzaKl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.post_processor = processors.TemplateProcessing(\n",
        "    single=\"$A:0 <sep>:0 <cls>:2\",\n",
        "    pair=\"$A:0 <sep>:0 $B:1 <sep>:1 <cls>:2\",\n",
        "    special_tokens=[(\"<sep>\", sep_token_id), (\"<cls>\", cls_token_id)],\n",
        ")"
      ],
      "metadata": {
        "id": "yaHKtkF5aRvn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoding = tokenizer.encode(\"Let's test this tokenizer...\", \"on a pair of sentences!\")\n",
        "print(encoding.tokens)\n",
        "print(encoding.type_ids)"
      ],
      "metadata": {
        "id": "6ZDt6W5eaTPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decoder = decoders.Metaspace()"
      ],
      "metadata": {
        "id": "bvHiCBazaWr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import XLNetTokenizerFast\n",
        "\n",
        "wrapped_tokenizer = XLNetTokenizerFast(tokenizer_object=tokenizer)"
      ],
      "metadata": {
        "id": "L_AZfJ_xaY5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ckd1oKBgbEu0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}