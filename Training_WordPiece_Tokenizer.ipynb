{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNbPuizR7Q6ndFZpcXJvW4I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Farbod-gsm99/NLP-Tasks-and-Tokenizers/blob/main/Training_WordPiece_Tokenizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "krwPIDtSPCK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9TCl9dWYFIUF"
      },
      "outputs": [],
      "source": [
        "from tokenizers import (\n",
        "    decoders,\n",
        "    models,\n",
        "    normalizers,\n",
        "    pre_tokenizers,\n",
        "    processors,\n",
        "    trainers,\n",
        "    Tokenizer,\n",
        ")\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"wikitext\", name=\"wikitext-2-raw-v1\", split=\"train\")\n",
        "\n",
        "\n",
        "def get_training_corpus():\n",
        "    for i in range(0, len(dataset), 1000):\n",
        "        yield dataset[i : i + 1000][\"text\"]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))\n",
        "\n",
        "tokenizer.normalizer = normalizers.Sequence(\n",
        "    [normalizers.NFD(), normalizers.Lowercase(), normalizers.StripAccents()]\n",
        ")"
      ],
      "metadata": {
        "id": "qO5DGqeAFbRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pre_tokenizer = pre_tokenizers.Sequence(\n",
        "    [pre_tokenizers.WhitespaceSplit(), pre_tokenizers.Punctuation()]\n",
        ")\n",
        "pre_tokenizer.pre_tokenize_str(\"Let's test my pre-tokenizer.\")"
      ],
      "metadata": {
        "id": "uiuzUc-qF9Ma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
        "trainer = trainers.WordPieceTrainer(vocab_size=25000, special_tokens=special_tokens)"
      ],
      "metadata": {
        "id": "4KrMUKbxGJaF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.train_from_iterator(get_training_corpus(), trainer = trainer)"
      ],
      "metadata": {
        "id": "QtZnNA2sKnRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoding = tokenizer.encode(\"Let's test this tokenizer.\")\n",
        "print(encoding.tokens)"
      ],
      "metadata": {
        "id": "0VifsimdK5kY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cls_token_id = tokenizer.token_to_id(\"[CLS]\")\n",
        "sep_token_id = tokenizer.token_to_id(\"[SEP]\")\n",
        "print(cls_token_id, sep_token_id)"
      ],
      "metadata": {
        "id": "GtmXqtKTK8Ec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.post_processor = processors.TemplateProcessing(\n",
        "    single=f\"[CLS]:0 $A:0 [SEP]:0\",\n",
        "    pair=f\"[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1\",\n",
        "    special_tokens=[(\"[CLS]\", cls_token_id), (\"[SEP]\", sep_token_id)],\n",
        ")"
      ],
      "metadata": {
        "id": "4npUn2_EK_eA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoding = tokenizer.encode(\"Let's test this tokenizer.\")\n",
        "print(encoding.tokens)"
      ],
      "metadata": {
        "id": "YGqILORmLiXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoding = tokenizer.encode(\"Let's test this tokenizer...\", \"on a pair of sentences.\")\n",
        "print(encoding.tokens)\n",
        "print(encoding.type_ids)"
      ],
      "metadata": {
        "id": "yNCq6s1dLlGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decoder = decoders.WordPiece(prefix=\"##\")"
      ],
      "metadata": {
        "id": "MDVqC9tjLpmh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(encoding.ids)"
      ],
      "metadata": {
        "id": "hOTjUJUdLwIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.save('tokenizer.json')"
      ],
      "metadata": {
        "id": "9wBxpoclL09I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login, login\n",
        "\n",
        "# For notebooks:\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "9CxURWY0SNWR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizerFast\n",
        "\n",
        "wrapped_tokenizer = BertTokenizerFast(tokenizer_object=tokenizer)"
      ],
      "metadata": {
        "id": "X7iWA86TRvmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wrapped_tokenizer.push_to_hub('WordPiece-Tokenizer')"
      ],
      "metadata": {
        "id": "VS3gV0vTS0Eo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QUnIP2P9S6Wh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}